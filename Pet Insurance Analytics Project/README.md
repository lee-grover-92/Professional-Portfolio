# ğŸ“˜ Pet Insurance Analytics Portfolio Project  
*A full end-to-end SQL + Python + BI project simulating a real-world pet insurance business*

## ğŸ¾ Project Summary  
This project simulates the operations of a UK-based pet insurance company, modelling customers, pets, policies, claims, and vet visits in a fully relational MySQL database. The data is synthetically generated using Python, with realistic business logic and referential integrity. The goal is to demonstrate the full analytics workflow â€” from data engineering and SQL analysis to business intelligence and insight storytelling â€” in a way that mirrors the expectations of a junior data analyst role.

---

## ğŸ¯ Project Objectives  
- Design a normalized relational schema for a pet insurance business  
- Generate realistic synthetic data using Python  
- Load and validate data in a MySQL environment  
- Write analytical SQL queries to extract business insights  
- Create processed, analysis-ready datasets  
- Explore data in Python (Pandas, Seaborn)  
- Build dashboards in Power BI or Tableau  
- Present findings in a clear, recruiter-friendly format  

---

## ğŸ› ï¸ Tech Stack  
- **MySQL** â€“ relational database design and querying  
- **Python** â€“ data generation and scripting  
- **Pandas / NumPy / Seaborn** â€“ analysis and visualization  
- **Power BI / Tableau** â€“ dashboards and reporting  
- **GitHub** â€“ version control and documentation  

---

## ğŸ—ƒï¸ Database Schema  
The schema models the core entities of a pet insurance provider:

| Table | Description |
|-------|-------------|
| `customers` | Customer demographic and contact details |
| `pets` | Pet profiles linked to customers |
| `products` | Insurance products with pricing and coverage |
| `policies` | Policy subscriptions per pet |
| `claims` | Claims submitted under policies |
| `vet_visits` | Vet appointments linked to claims |
| `invoice_line_items` | Itemized treatment costs per visit |
| `claim_payments` | Payments issued for approved claims |

---

## ğŸ” Key Business Questions  
This dataset enables analysis of real-world insurance KPIs and customer behaviour:

- Which pet breeds generate the highest average claim costs?  
- How does pet age affect policy risk and claim frequency?  
- What is the loss ratio by product and coverage type?  
- What is the average customer lifetime value (CLV)?  
- Which vet clinics are most expensive or most visited?  
- What is the claim approval rate by diagnosis?  
- How many customers own multiple pets or policies?  
- What is the average time from claim to payment?

---

## ğŸ§± Project Structure  
```
pet_data_project/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                            # Synthetic CSVs for each table
â”‚   â””â”€â”€ processed/                      # Final exports for analysis or BI
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ create_db.sql                   # Schema creation
â”‚   â”œâ”€â”€ batch_load_csv.sql              # Bulk CSV import
â”‚   â”œâ”€â”€ data_overview_queries.sql       # Initial exploration
â”‚   â”œâ”€â”€ integrity_checks.sql            # FK and data validation
â”‚   â””â”€â”€ analysis_queries.sql            # Business-focused SQL analysis
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ fake_insurance_data_creator.py  # Python data generator
â”‚   â”œâ”€â”€ export_views_to_csv.py          # Exports SQL views to CSV
â”‚   â””â”€â”€ sql_server_settings.bat         # Local MySQL config
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ exploratory_analysis.ipynb
â”‚   â”œâ”€â”€ claims_risk_modelling.ipynb
â”‚   â””â”€â”€ customer_lifetime_value.ipynb
â”‚
â”œâ”€â”€ dashboards/
â”‚   â””â”€â”€ tableau/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ schema_diagram.png
â”‚   â”œâ”€â”€ data_dictionary.md
â”‚   â””â”€â”€ project_overview.pdf
â”‚
â””â”€â”€ images/
    â””â”€â”€ dashboard_screenshots/
```

---

## âš™ï¸ How to Run Locally

1. **Create the database**  
   ```bash
   mysql -u root -p < sql/create_db.sql
   ```

2. **Load the data**  
   ```bash
   mysql -u root -p < sql/batch_load_csv.sql
   ```

3. **Explore the data**  
   Run `sql/data_overview_queries.sql` to get a feel for the dataset.

4. **Run analysis**  
   Use the Jupyter notebooks in `/notebooks` or execute SQL from `analysis_queries.sql`.

5. **Export processed views to CSV**  
   ```bash
   cd scripts/
   python export_views_to_csv.py
   ```
   This will save full CSVs to `data/processed/` for use in dashboards or Python notebooks.

6. **Build dashboards**  
   Connect Power BI or Tableau to your processed CSVs or MySQL views and build visuals.

---